{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pymilvus import connections, FieldSchema, CollectionSchema, DataType, Collection, utility\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch import clamp, sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [],
   "source": [
    "DIMENSION = 768  # Embeddings size\n",
    "MODEL = 'bert-base-uncased'  # Transformer to use for embeddings"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [],
   "source": [
    "connections.connect(host='127.0.0.1', port='19530')\n",
    "\n",
    "if utility.has_collection('huggingface_db'):\n",
    "    utility.drop_collection('huggingface_db')\n",
    "\n",
    "fields = [\n",
    "    FieldSchema(name='id', dtype=DataType.INT64, is_primary=True, auto_id=True),\n",
    "    FieldSchema(name='original_question', dtype=DataType.VARCHAR, max_length=1000),\n",
    "    FieldSchema(name='answer', dtype=DataType.VARCHAR, max_length=1000),\n",
    "    FieldSchema(name='original_question_embedding', dtype=DataType.FLOAT_VECTOR, dim=DIMENSION)\n",
    "]\n",
    "schema = CollectionSchema(fields=fields)\n",
    "collection = Collection(name='huggingface_db', schema=schema)\n",
    "\n",
    "index_params = {\n",
    "    'metric_type': 'L2',\n",
    "    'index_type': \"IVF_FLAT\",\n",
    "    'params': {\"nlist\": 1536}\n",
    "}\n",
    "collection.create_index(field_name=\"original_question_embedding\", index_params=index_params)\n",
    "collection.load()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from C:\\Users\\duanm\\.cache\\huggingface\\modules\\datasets_modules\\datasets\\squad\\d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453 (last modified on Mon Jul 10 10:42:33 2023) since it couldn't be found locally at squad., or remotely on the Hugging Face Hub.\n",
      "Found cached dataset squad (C:/Users/duanm/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n",
      "Loading cached split indices for dataset at C:\\Users\\duanm\\.cache\\huggingface\\datasets\\squad\\plain_text\\1.0.0\\d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453\\cache-3f8f639b545735e4.arrow and C:\\Users\\duanm\\.cache\\huggingface\\datasets\\squad\\plain_text\\1.0.0\\d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453\\cache-8f60fd09160cab3e.arrow\n",
      "Loading cached processed dataset at C:\\Users\\duanm\\.cache\\huggingface\\datasets\\squad\\plain_text\\1.0.0\\d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453\\cache-05d7f211fad350ed.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": "Dataset({\n    features: ['id', 'title', 'context', 'question', 'answer'],\n    num_rows: 99\n})"
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dataset = load_dataset('squad', split='all')\n",
    "data_dataset = data_dataset.train_test_split(test_size=.001, seed=42)['test']\n",
    "data_dataset = data_dataset.map(lambda val: {'answer': val['answers']['text'][0]}, remove_columns=['answers'])\n",
    "data_dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\duanm\\.cache\\huggingface\\datasets\\squad\\plain_text\\1.0.0\\d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453\\cache-df4165deb4a6611f.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": "Dataset({\n    features: ['id', 'title', 'context', 'question', 'answer', 'input_ids', 'token_type_ids', 'attention_mask'],\n    num_rows: 99\n})"
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "\n",
    "\n",
    "def tokenize_question(batch):\n",
    "    results = tokenizer(batch['question'], add_special_tokens=True, truncation=True, padding=\"max_length\",\n",
    "                        return_attention_mask=True, return_tensors=\"pt\")\n",
    "    batch['input_ids'] = results['input_ids']\n",
    "    batch['token_type_ids'] = results['token_type_ids']\n",
    "    batch['attention_mask'] = results['attention_mask']\n",
    "    return batch\n",
    "\n",
    "\n",
    "data_dataset = data_dataset.map(tokenize_question, batch_size=1000, batched=True)\n",
    "data_dataset.set_format('torch', columns=['input_ids', 'token_type_ids', 'attention_mask'], output_all_columns=True)\n",
    "data_dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Loading cached processed dataset at C:\\Users\\duanm\\.cache\\huggingface\\datasets\\squad\\plain_text\\1.0.0\\d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453\\cache-5edcd21208f03274.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": "Dataset({\n    features: ['id', 'title', 'context', 'question', 'answer', 'input_ids', 'token_type_ids', 'attention_mask', 'question_embedding'],\n    num_rows: 99\n})"
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained(MODEL)\n",
    "\n",
    "\n",
    "def embed(batch):\n",
    "    sentence_embs = model(\n",
    "        input_ids=batch['input_ids'],\n",
    "        token_type_ids=batch['token_type_ids'],\n",
    "        attention_mask=batch['attention_mask']\n",
    "    )[0]\n",
    "    input_mask_expanded = batch['attention_mask'].unsqueeze(-1).expand(sentence_embs.size()).float()\n",
    "    batch['question_embedding'] = sum(sentence_embs * input_mask_expanded, 1) / clamp(input_mask_expanded.sum(1),\n",
    "                                                                                      min=1e-9)\n",
    "    return batch\n",
    "\n",
    "\n",
    "data_dataset = data_dataset.map(embed, remove_columns=['input_ids', 'token_type_ids', 'attention_mask'], batched=True,\n",
    "                                batch_size=64)\n",
    "data_dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/99 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "147f27f1f82d4116a557a3c5385c850a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Dataset({\n    features: ['id', 'title', 'context', 'question', 'answer', 'input_ids', 'token_type_ids', 'attention_mask', 'question_embedding'],\n    num_rows: 99\n})"
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def insert_function(batch):\n",
    "    insertable = [\n",
    "        batch['question'],\n",
    "        [x[:995] + '...' if len(x) > 999 else x for x in batch['answer']],\n",
    "        batch['question_embedding'].tolist()\n",
    "    ]\n",
    "    collection.insert(insertable)\n",
    "\n",
    "\n",
    "data_dataset.map(insert_function, batched=True, batch_size=64)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/2 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c7b0be55cf434976b71c88ac065dd0b8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/2 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1edcc66674744f6e8235711cc2781368"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Dataset({\n    features: ['question', 'input_ids', 'token_type_ids', 'attention_mask', 'question_embedding'],\n    num_rows: 2\n})"
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions = {'question': ['When was chemistry invented?', 'When was Eisenhower born?']}\n",
    "question_dataset = Dataset.from_dict(questions)\n",
    "question_dataset = question_dataset.map(tokenize_question, batched=True, batch_size=1000)\n",
    "question_dataset.set_format('torch', columns=['input_ids', 'token_type_ids', 'attention_mask'], output_all_columns=True)\n",
    "question_dataset = question_dataset.map(embed, remove_columns=['input_ids', 'token_type_ids', 'attention_mask'],\n",
    "                                        batched=True, batch_size=64)\n",
    "question_dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/2 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5dd6d24e44c74ae98189603f39ea8f69"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When was chemistry invented?\n",
      "('When did the Papal States exist?', 'until 1870', 34.13431167602539)\n",
      "('When was the Tower constructed?', '1787', 35.67085266113281)\n",
      "('When were free elections held?', 'October 1992', 38.95990753173828)\n",
      "('How old did biblical scholars think the Earth was?', '6,000 years', 44.86854553222656)\n",
      "('Where was Russian schooling mandatory in the 20th century?', 'Poland, Bulgaria, the Czech Republic, Slovakia, Hungary, Albania, former East Germany and Cuba', 45.79857635498047)\n",
      "('In what year was the Premier League created?', '1992', 47.060733795166016)\n",
      "(\"When was ZE's Mutant Disco released?\", '1981', 48.399925231933594)\n",
      "(\"What was the Latin of Charlemagne's era later known as?\", 'Medieval Latin', 50.96128845214844)\n",
      "('How did Hobson argue to rid the world of imperialism?', 'taxation', 51.08031463623047)\n",
      "('What Prussian system was superior to the French example?', 'military education', 52.56195068359375)\n",
      "\n",
      "When was Eisenhower born?\n",
      "('When was the Tower constructed?', '1787', 32.51980972290039)\n",
      "('In what year was the Premier League created?', '1992', 39.181663513183594)\n",
      "('When were free elections held?', 'October 1992', 39.4478874206543)\n",
      "('When did the Papal States exist?', 'until 1870', 42.285667419433594)\n",
      "(\"When was ZE's Mutant Disco released?\", '1981', 46.21398162841797)\n",
      "('What issue did Spielberg address in his movie Munich?', 'terrorism', 47.57118225097656)\n",
      "('In what year did the State Management Scheme cease?', '1973', 48.69563293457031)\n",
      "('What feature originally covered Alberta?', 'ocean', 52.243934631347656)\n",
      "('What did Newcastle University win in 2000?', 'Sunday Times University of the Year award', 52.623085021972656)\n",
      "('Where was Russian schooling mandatory in the 20th century?', 'Poland, Bulgaria, the Czech Republic, Slovakia, Hungary, Albania, former East Germany and Cuba', 53.21949768066406)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def search(batch):\n",
    "    res = collection.search(batch['question_embedding'].tolist(),\n",
    "                            anns_field='original_question_embedding', param={},\n",
    "                            limit=10,\n",
    "                            output_fields=['answer', 'original_question'])\n",
    "\n",
    "    overall_id = []\n",
    "    overall_distance = []\n",
    "    overall_answer = []\n",
    "    overall_original_question = []\n",
    "    for hits in res:\n",
    "        ids = []\n",
    "        distance = []\n",
    "        answer = []\n",
    "        original_question = []\n",
    "        for hit in hits:\n",
    "            ids.append(hit.id)\n",
    "            distance.append(hit.distance)\n",
    "            answer.append(hit.entity.get('answer'))\n",
    "            original_question.append(hit.entity.get('original_question'))\n",
    "        overall_id.append(ids)\n",
    "        overall_distance.append(distance)\n",
    "        overall_answer.append(answer)\n",
    "        overall_original_question.append(original_question)\n",
    "\n",
    "    return {'id': overall_id,\n",
    "            'distance': overall_distance,\n",
    "            'answer': overall_answer,\n",
    "            'original_question': overall_original_question}\n",
    "\n",
    "\n",
    "question_dataset = question_dataset.map(search, batched=True, batch_size=1)\n",
    "for x in question_dataset:\n",
    "    print(x['question'])\n",
    "    for x1, x2, x3 in zip(x['original_question'], x['answer'], x['distance']):\n",
    "        print((x1, x2, x3.item()))\n",
    "    print()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [],
   "source": [
    "collection.release()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}