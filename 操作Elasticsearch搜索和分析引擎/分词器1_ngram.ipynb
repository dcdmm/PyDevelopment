{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "es = Elasticsearch([{'host': 'localhost', 'port': 9200}])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "{'tokens': [{'token': 'Q',\n   'start_offset': 0,\n   'end_offset': 1,\n   'type': 'word',\n   'position': 0},\n  {'token': 'Qu',\n   'start_offset': 0,\n   'end_offset': 2,\n   'type': 'word',\n   'position': 1},\n  {'token': 'u',\n   'start_offset': 1,\n   'end_offset': 2,\n   'type': 'word',\n   'position': 2},\n  {'token': 'ui',\n   'start_offset': 1,\n   'end_offset': 3,\n   'type': 'word',\n   'position': 3},\n  {'token': 'i',\n   'start_offset': 2,\n   'end_offset': 3,\n   'type': 'word',\n   'position': 4},\n  {'token': 'ic',\n   'start_offset': 2,\n   'end_offset': 4,\n   'type': 'word',\n   'position': 5},\n  {'token': 'c',\n   'start_offset': 3,\n   'end_offset': 4,\n   'type': 'word',\n   'position': 6},\n  {'token': 'ck',\n   'start_offset': 3,\n   'end_offset': 5,\n   'type': 'word',\n   'position': 7},\n  {'token': 'k',\n   'start_offset': 4,\n   'end_offset': 5,\n   'type': 'word',\n   'position': 8},\n  {'token': 'k ',\n   'start_offset': 4,\n   'end_offset': 6,\n   'type': 'word',\n   'position': 9},\n  {'token': ' ',\n   'start_offset': 5,\n   'end_offset': 6,\n   'type': 'word',\n   'position': 10},\n  {'token': ' F',\n   'start_offset': 5,\n   'end_offset': 7,\n   'type': 'word',\n   'position': 11},\n  {'token': 'F',\n   'start_offset': 6,\n   'end_offset': 7,\n   'type': 'word',\n   'position': 12},\n  {'token': 'Fo',\n   'start_offset': 6,\n   'end_offset': 8,\n   'type': 'word',\n   'position': 13},\n  {'token': 'o',\n   'start_offset': 7,\n   'end_offset': 8,\n   'type': 'word',\n   'position': 14},\n  {'token': 'ox',\n   'start_offset': 7,\n   'end_offset': 9,\n   'type': 'word',\n   'position': 15},\n  {'token': 'x',\n   'start_offset': 8,\n   'end_offset': 9,\n   'type': 'word',\n   'position': 16}]}"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The ngram tokenizer first breaks text down into words whenever it encounters one of a list of specified characters, then it emits N-grams of each word of the specified length.\n",
    "es.indices.analyze(\n",
    "    body={\n",
    "        \"tokenizer\": \"ngram\",\n",
    "        \"text\": \"Quick Fox\"\n",
    "    })"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "{'tokens': [{'token': 'Qu',\n   'start_offset': 0,\n   'end_offset': 2,\n   'type': 'word',\n   'position': 0},\n  {'token': 'Qui',\n   'start_offset': 0,\n   'end_offset': 3,\n   'type': 'word',\n   'position': 1},\n  {'token': 'ui',\n   'start_offset': 1,\n   'end_offset': 3,\n   'type': 'word',\n   'position': 2},\n  {'token': 'uic',\n   'start_offset': 1,\n   'end_offset': 4,\n   'type': 'word',\n   'position': 3},\n  {'token': 'ic',\n   'start_offset': 2,\n   'end_offset': 4,\n   'type': 'word',\n   'position': 4},\n  {'token': 'ick',\n   'start_offset': 2,\n   'end_offset': 5,\n   'type': 'word',\n   'position': 5},\n  {'token': 'ck',\n   'start_offset': 3,\n   'end_offset': 5,\n   'type': 'word',\n   'position': 6},\n  {'token': 'ck ',\n   'start_offset': 3,\n   'end_offset': 6,\n   'type': 'word',\n   'position': 7},\n  {'token': 'k ',\n   'start_offset': 4,\n   'end_offset': 6,\n   'type': 'word',\n   'position': 8},\n  {'token': 'k F',\n   'start_offset': 4,\n   'end_offset': 7,\n   'type': 'word',\n   'position': 9},\n  {'token': ' F',\n   'start_offset': 5,\n   'end_offset': 7,\n   'type': 'word',\n   'position': 10},\n  {'token': ' Fo',\n   'start_offset': 5,\n   'end_offset': 8,\n   'type': 'word',\n   'position': 11},\n  {'token': 'Fo',\n   'start_offset': 6,\n   'end_offset': 8,\n   'type': 'word',\n   'position': 12},\n  {'token': 'Fox',\n   'start_offset': 6,\n   'end_offset': 9,\n   'type': 'word',\n   'position': 13},\n  {'token': 'ox',\n   'start_offset': 7,\n   'end_offset': 9,\n   'type': 'word',\n   'position': 14}]}"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "min_gram\n",
    "    Minimum length of characters in a gram. Defaults to 1.\n",
    "\n",
    "max_gram\n",
    "    Maximum length of characters in a gram. Defaults to 2.\n",
    "'''\n",
    "\n",
    "es.indices.analyze(\n",
    "    body={\n",
    "        \"tokenizer\": {\n",
    "            \"type\": \"ngram\",\n",
    "            \"min_gram\": 2,\n",
    "            \"max_gram\": 3\n",
    "        },\n",
    "        \"text\": \"Quick Fox\"})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "ename": "RequestError",
     "evalue": "RequestError(400, 'illegal_argument_exception', 'The difference between max_gram and min_gram in NGram Tokenizer must be less than or equal to: [1] but was [2]. This limit can be set by changing the [index.max_ngram_diff] index level setting.')",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRequestError\u001B[0m                              Traceback (most recent call last)",
      "Input \u001B[1;32mIn [17]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mes\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mindices\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43manalyze\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m      2\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbody\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m{\u001B[49m\n\u001B[0;32m      3\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtokenizer\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43m{\u001B[49m\n\u001B[0;32m      4\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtype\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mngram\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m      5\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmin_gram\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m      6\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmax_gram\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3\u001B[39;49m\n\u001B[0;32m      7\u001B[0m \u001B[43m        \u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      8\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtext\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mQuick Fox\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\elasticsearch\\client\\utils.py:152\u001B[0m, in \u001B[0;36mquery_params.<locals>._wrapper.<locals>._wrapped\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    150\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m p \u001B[38;5;129;01min\u001B[39;00m kwargs:\n\u001B[0;32m    151\u001B[0m         params[p] \u001B[38;5;241m=\u001B[39m kwargs\u001B[38;5;241m.\u001B[39mpop(p)\n\u001B[1;32m--> 152\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparams\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\elasticsearch\\client\\indices.py:34\u001B[0m, in \u001B[0;36mIndicesClient.analyze\u001B[1;34m(self, body, index, params, headers)\u001B[0m\n\u001B[0;32m     22\u001B[0m \u001B[38;5;129m@query_params\u001B[39m()\n\u001B[0;32m     23\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21manalyze\u001B[39m(\u001B[38;5;28mself\u001B[39m, body\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, index\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, params\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, headers\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m     24\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     25\u001B[0m \u001B[38;5;124;03m    Performs the analysis process on a text and return the tokens breakdown of the\u001B[39;00m\n\u001B[0;32m     26\u001B[0m \u001B[38;5;124;03m    text.\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     32\u001B[0m \u001B[38;5;124;03m    :arg index: The name of the index to scope the operation\u001B[39;00m\n\u001B[0;32m     33\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m---> 34\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransport\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mperform_request\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     35\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mPOST\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     36\u001B[0m \u001B[43m        \u001B[49m\u001B[43m_make_path\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m_analyze\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     37\u001B[0m \u001B[43m        \u001B[49m\u001B[43mparams\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     38\u001B[0m \u001B[43m        \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     39\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbody\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbody\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     40\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\elasticsearch\\transport.py:415\u001B[0m, in \u001B[0;36mTransport.perform_request\u001B[1;34m(self, method, url, headers, params, body)\u001B[0m\n\u001B[0;32m    413\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[0;32m    414\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 415\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[0;32m    417\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    418\u001B[0m     \u001B[38;5;66;03m# connection didn't fail, confirm it's live status\u001B[39;00m\n\u001B[0;32m    419\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconnection_pool\u001B[38;5;241m.\u001B[39mmark_live(connection)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\elasticsearch\\transport.py:381\u001B[0m, in \u001B[0;36mTransport.perform_request\u001B[1;34m(self, method, url, headers, params, body)\u001B[0m\n\u001B[0;32m    378\u001B[0m connection \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_connection()\n\u001B[0;32m    380\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 381\u001B[0m     status, headers_response, data \u001B[38;5;241m=\u001B[39m \u001B[43mconnection\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mperform_request\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    382\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    383\u001B[0m \u001B[43m        \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    384\u001B[0m \u001B[43m        \u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    385\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbody\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    386\u001B[0m \u001B[43m        \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    387\u001B[0m \u001B[43m        \u001B[49m\u001B[43mignore\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mignore\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    388\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    389\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    391\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m TransportError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    392\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m method \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mHEAD\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m e\u001B[38;5;241m.\u001B[39mstatus_code \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m404\u001B[39m:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\elasticsearch\\connection\\http_urllib3.py:273\u001B[0m, in \u001B[0;36mUrllib3HttpConnection.perform_request\u001B[1;34m(self, method, url, params, body, timeout, ignore, headers)\u001B[0m\n\u001B[0;32m    269\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;241m200\u001B[39m \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m response\u001B[38;5;241m.\u001B[39mstatus \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m300\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m response\u001B[38;5;241m.\u001B[39mstatus \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m ignore:\n\u001B[0;32m    270\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlog_request_fail(\n\u001B[0;32m    271\u001B[0m         method, full_url, url, orig_body, duration, response\u001B[38;5;241m.\u001B[39mstatus, raw_data\n\u001B[0;32m    272\u001B[0m     )\n\u001B[1;32m--> 273\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_raise_error\u001B[49m\u001B[43m(\u001B[49m\u001B[43mresponse\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstatus\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mraw_data\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    275\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlog_request_success(\n\u001B[0;32m    276\u001B[0m     method, full_url, url, orig_body, response\u001B[38;5;241m.\u001B[39mstatus, raw_data, duration\n\u001B[0;32m    277\u001B[0m )\n\u001B[0;32m    279\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m response\u001B[38;5;241m.\u001B[39mstatus, response\u001B[38;5;241m.\u001B[39mgetheaders(), raw_data\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\elasticsearch\\connection\\base.py:322\u001B[0m, in \u001B[0;36mConnection._raise_error\u001B[1;34m(self, status_code, raw_data)\u001B[0m\n\u001B[0;32m    319\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (\u001B[38;5;167;01mValueError\u001B[39;00m, \u001B[38;5;167;01mTypeError\u001B[39;00m) \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[0;32m    320\u001B[0m     logger\u001B[38;5;241m.\u001B[39mwarning(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUndecodable raw error response from server: \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m, err)\n\u001B[1;32m--> 322\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m HTTP_EXCEPTIONS\u001B[38;5;241m.\u001B[39mget(status_code, TransportError)(\n\u001B[0;32m    323\u001B[0m     status_code, error_message, additional_info\n\u001B[0;32m    324\u001B[0m )\n",
      "\u001B[1;31mRequestError\u001B[0m: RequestError(400, 'illegal_argument_exception', 'The difference between max_gram and min_gram in NGram Tokenizer must be less than or equal to: [1] but was [2]. This limit can be set by changing the [index.max_ngram_diff] index level setting.')"
     ]
    }
   ],
   "source": [
    "es.indices.analyze(\n",
    "    body={\n",
    "        \"tokenizer\": {\n",
    "            \"type\": \"ngram\",\n",
    "\n",
    "            # 报错:RequestError: RequestError(400, 'illegal_argument_exception', 'The difference between max_gram and min_gram in NGram Tokenizer must be less than or equal to: [1] but was [2]. This limit can be set by changing the [index.max_ngram_diff] index level setting.')\n",
    "            \"min_gram\": 1,\n",
    "            \"max_gram\": 3\n",
    "        },\n",
    "        \"text\": \"Quick Fox\"})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "{'tokens': [{'token': '我来自',\n   'start_offset': 0,\n   'end_offset': 3,\n   'type': 'word',\n   'position': 0},\n  {'token': '来自中',\n   'start_offset': 1,\n   'end_offset': 4,\n   'type': 'word',\n   'position': 1},\n  {'token': '自中国',\n   'start_offset': 2,\n   'end_offset': 5,\n   'type': 'word',\n   'position': 2},\n  {'token': '中国!',\n   'start_offset': 3,\n   'end_offset': 6,\n   'type': 'word',\n   'position': 3}]}"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es.indices.analyze(\n",
    "    body={\n",
    "        \"tokenizer\": {\n",
    "            \"type\": \"ngram\",\n",
    "\n",
    "            # 3元组\n",
    "            \"min_gram\": 3,\n",
    "            \"max_gram\": 3\n",
    "        },\n",
    "        \"text\": \"我来自中国!\"})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}